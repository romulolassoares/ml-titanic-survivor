{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/titanic.pkl', 'rb') as f:\n",
    "   X_titanic_train, y_titanic_train, X_titanic_test, y_titanic_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 21), (891,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_titanic_train.shape, y_titanic_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((418, 21), (418,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_titanic_test.shape, y_titanic_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_titanic = np.concatenate((X_titanic_train, X_titanic_test), axis=0)\n",
    "y_titanic = np.concatenate((y_titanic_train, y_titanic_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'activation': ['relu', 'logistic', 'tanh'],\n",
    "              'solver': ['adam', 'sgd'],\n",
    "              'hidden_layer_sizes': [(8,), (8,8), (8,8,8)],\n",
    "              'tol': [0.00010, 0.000010, 0.0000010]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param: {'activation': 'logistic', 'hidden_layer_sizes': (8, 8, 8), 'solver': 'adam', 'tol': 1e-06} and Best Socore: 0.85951566201632\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=MLPClassifier(max_iter=10000, tol=0.000010), param_grid=params)\n",
    "grid_search.fit(X_titanic, y_titanic)\n",
    "best_param = grid_search.best_params_\n",
    "best_result = grid_search.best_score_\n",
    "print(\"Best Param: {} and Best Socore: {}\".format(best_param, best_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72467870\n",
      "Iteration 2, loss = 0.71784374\n",
      "Iteration 3, loss = 0.71162305\n",
      "Iteration 4, loss = 0.70568949\n",
      "Iteration 5, loss = 0.70051693\n",
      "Iteration 6, loss = 0.69501406\n",
      "Iteration 7, loss = 0.69090092\n",
      "Iteration 8, loss = 0.68671556\n",
      "Iteration 9, loss = 0.68313445\n",
      "Iteration 10, loss = 0.67956046\n",
      "Iteration 11, loss = 0.67672224\n",
      "Iteration 12, loss = 0.67429770\n",
      "Iteration 13, loss = 0.67176737\n",
      "Iteration 14, loss = 0.66962151\n",
      "Iteration 15, loss = 0.66787507\n",
      "Iteration 16, loss = 0.66643652\n",
      "Iteration 17, loss = 0.66483853\n",
      "Iteration 18, loss = 0.66357494\n",
      "Iteration 19, loss = 0.66243494\n",
      "Iteration 20, loss = 0.66159567\n",
      "Iteration 21, loss = 0.66064587\n",
      "Iteration 22, loss = 0.65977527\n",
      "Iteration 23, loss = 0.65903006\n",
      "Iteration 24, loss = 0.65825916\n",
      "Iteration 25, loss = 0.65753459\n",
      "Iteration 26, loss = 0.65680249\n",
      "Iteration 27, loss = 0.65611356\n",
      "Iteration 28, loss = 0.65547703\n",
      "Iteration 29, loss = 0.65479835\n",
      "Iteration 30, loss = 0.65414012\n",
      "Iteration 31, loss = 0.65349115\n",
      "Iteration 32, loss = 0.65283490\n",
      "Iteration 33, loss = 0.65214884\n",
      "Iteration 34, loss = 0.65149040\n",
      "Iteration 35, loss = 0.65071377\n",
      "Iteration 36, loss = 0.65000534\n",
      "Iteration 37, loss = 0.64923955\n",
      "Iteration 38, loss = 0.64847167\n",
      "Iteration 39, loss = 0.64765875\n",
      "Iteration 40, loss = 0.64689135\n",
      "Iteration 41, loss = 0.64607867\n",
      "Iteration 42, loss = 0.64519444\n",
      "Iteration 43, loss = 0.64430297\n",
      "Iteration 44, loss = 0.64341359\n",
      "Iteration 45, loss = 0.64253603\n",
      "Iteration 46, loss = 0.64192941\n",
      "Iteration 47, loss = 0.64061175\n",
      "Iteration 48, loss = 0.63970917\n",
      "Iteration 49, loss = 0.63868770\n",
      "Iteration 50, loss = 0.63756117\n",
      "Iteration 51, loss = 0.63645286\n",
      "Iteration 52, loss = 0.63538960\n",
      "Iteration 53, loss = 0.63435587\n",
      "Iteration 54, loss = 0.63312851\n",
      "Iteration 55, loss = 0.63191212\n",
      "Iteration 56, loss = 0.63070389\n",
      "Iteration 57, loss = 0.62941597\n",
      "Iteration 58, loss = 0.62807792\n",
      "Iteration 59, loss = 0.62680224\n",
      "Iteration 60, loss = 0.62549693\n",
      "Iteration 61, loss = 0.62419730\n",
      "Iteration 62, loss = 0.62285578\n",
      "Iteration 63, loss = 0.62129644\n",
      "Iteration 64, loss = 0.61979483\n",
      "Iteration 65, loss = 0.61838608\n",
      "Iteration 66, loss = 0.61678342\n",
      "Iteration 67, loss = 0.61523603\n",
      "Iteration 68, loss = 0.61366215\n",
      "Iteration 69, loss = 0.61205723\n",
      "Iteration 70, loss = 0.61049312\n",
      "Iteration 71, loss = 0.60877785\n",
      "Iteration 72, loss = 0.60704182\n",
      "Iteration 73, loss = 0.60529470\n",
      "Iteration 74, loss = 0.60361181\n",
      "Iteration 75, loss = 0.60170629\n",
      "Iteration 76, loss = 0.59994930\n",
      "Iteration 77, loss = 0.59814073\n",
      "Iteration 78, loss = 0.59634134\n",
      "Iteration 79, loss = 0.59422232\n",
      "Iteration 80, loss = 0.59228636\n",
      "Iteration 81, loss = 0.59035035\n",
      "Iteration 82, loss = 0.58797145\n",
      "Iteration 83, loss = 0.58612633\n",
      "Iteration 84, loss = 0.58343314\n",
      "Iteration 85, loss = 0.58174937\n",
      "Iteration 86, loss = 0.57900100\n",
      "Iteration 87, loss = 0.57623894\n",
      "Iteration 88, loss = 0.57351758\n",
      "Iteration 89, loss = 0.57071060\n",
      "Iteration 90, loss = 0.56787328\n",
      "Iteration 91, loss = 0.56487709\n",
      "Iteration 92, loss = 0.56205365\n",
      "Iteration 93, loss = 0.55903540\n",
      "Iteration 94, loss = 0.55590466\n",
      "Iteration 95, loss = 0.55309350\n",
      "Iteration 96, loss = 0.55007583\n",
      "Iteration 97, loss = 0.54828885\n",
      "Iteration 98, loss = 0.54491337\n",
      "Iteration 99, loss = 0.54146315\n",
      "Iteration 100, loss = 0.53826659\n",
      "Iteration 101, loss = 0.53533187\n",
      "Iteration 102, loss = 0.53208869\n",
      "Iteration 103, loss = 0.52961581\n",
      "Iteration 104, loss = 0.52677650\n",
      "Iteration 105, loss = 0.52320831\n",
      "Iteration 106, loss = 0.52118121\n",
      "Iteration 107, loss = 0.51821156\n",
      "Iteration 108, loss = 0.51479780\n",
      "Iteration 109, loss = 0.51358294\n",
      "Iteration 110, loss = 0.50958163\n",
      "Iteration 111, loss = 0.50883460\n",
      "Iteration 112, loss = 0.50312150\n",
      "Iteration 113, loss = 0.50393551\n",
      "Iteration 114, loss = 0.49863442\n",
      "Iteration 115, loss = 0.49795395\n",
      "Iteration 116, loss = 0.49473985\n",
      "Iteration 117, loss = 0.49265356\n",
      "Iteration 118, loss = 0.48976358\n",
      "Iteration 119, loss = 0.48726646\n",
      "Iteration 120, loss = 0.48577634\n",
      "Iteration 121, loss = 0.48273485\n",
      "Iteration 122, loss = 0.48278892\n",
      "Iteration 123, loss = 0.47943037\n",
      "Iteration 124, loss = 0.47701013\n",
      "Iteration 125, loss = 0.47378207\n",
      "Iteration 126, loss = 0.47193922\n",
      "Iteration 127, loss = 0.47022137\n",
      "Iteration 128, loss = 0.46975077\n",
      "Iteration 129, loss = 0.46729304\n",
      "Iteration 130, loss = 0.46498202\n",
      "Iteration 131, loss = 0.46317586\n",
      "Iteration 132, loss = 0.46167078\n",
      "Iteration 133, loss = 0.46071334\n",
      "Iteration 134, loss = 0.45937935\n",
      "Iteration 135, loss = 0.45791412\n",
      "Iteration 136, loss = 0.45614886\n",
      "Iteration 137, loss = 0.45539837\n",
      "Iteration 138, loss = 0.45534369\n",
      "Iteration 139, loss = 0.45171887\n",
      "Iteration 140, loss = 0.45191648\n",
      "Iteration 141, loss = 0.45102511\n",
      "Iteration 142, loss = 0.44923774\n",
      "Iteration 143, loss = 0.44784762\n",
      "Iteration 144, loss = 0.44675911\n",
      "Iteration 145, loss = 0.44764459\n",
      "Iteration 146, loss = 0.44443382\n",
      "Iteration 147, loss = 0.44443949\n",
      "Iteration 148, loss = 0.44301979\n",
      "Iteration 149, loss = 0.44329048\n",
      "Iteration 150, loss = 0.44147122\n",
      "Iteration 151, loss = 0.44100860\n",
      "Iteration 152, loss = 0.44018075\n",
      "Iteration 153, loss = 0.43912410\n",
      "Iteration 154, loss = 0.43921714\n",
      "Iteration 155, loss = 0.43830047\n",
      "Iteration 156, loss = 0.43732784\n",
      "Iteration 157, loss = 0.43754677\n",
      "Iteration 158, loss = 0.43580677\n",
      "Iteration 159, loss = 0.43526814\n",
      "Iteration 160, loss = 0.43496369\n",
      "Iteration 161, loss = 0.43407546\n",
      "Iteration 162, loss = 0.43360686\n",
      "Iteration 163, loss = 0.43310455\n",
      "Iteration 164, loss = 0.43284079\n",
      "Iteration 165, loss = 0.43370508\n",
      "Iteration 166, loss = 0.43216123\n",
      "Iteration 167, loss = 0.43210790\n",
      "Iteration 168, loss = 0.43051200\n",
      "Iteration 169, loss = 0.43169923\n",
      "Iteration 170, loss = 0.43050779\n",
      "Iteration 171, loss = 0.43187405\n",
      "Iteration 172, loss = 0.42880850\n",
      "Iteration 173, loss = 0.42994998\n",
      "Iteration 174, loss = 0.42808626\n",
      "Iteration 175, loss = 0.42808856\n",
      "Iteration 176, loss = 0.43099017\n",
      "Iteration 177, loss = 0.42872797\n",
      "Iteration 178, loss = 0.42752080\n",
      "Iteration 179, loss = 0.42750676\n",
      "Iteration 180, loss = 0.42641762\n",
      "Iteration 181, loss = 0.42578920\n",
      "Iteration 182, loss = 0.42604177\n",
      "Iteration 183, loss = 0.42627793\n",
      "Iteration 184, loss = 0.42518661\n",
      "Iteration 185, loss = 0.42438896\n",
      "Iteration 186, loss = 0.42378076\n",
      "Iteration 187, loss = 0.42438288\n",
      "Iteration 188, loss = 0.42380673\n",
      "Iteration 189, loss = 0.42324121\n",
      "Iteration 190, loss = 0.42274364\n",
      "Iteration 191, loss = 0.42213147\n",
      "Iteration 192, loss = 0.42272334\n",
      "Iteration 193, loss = 0.42363849\n",
      "Iteration 194, loss = 0.42221246\n",
      "Iteration 195, loss = 0.42258709\n",
      "Iteration 196, loss = 0.42195067\n",
      "Iteration 197, loss = 0.42070746\n",
      "Iteration 198, loss = 0.42139718\n",
      "Iteration 199, loss = 0.42060977\n",
      "Iteration 200, loss = 0.42047345\n",
      "Iteration 201, loss = 0.41980425\n",
      "Iteration 202, loss = 0.42024871\n",
      "Iteration 203, loss = 0.41973497\n",
      "Iteration 204, loss = 0.42017322\n",
      "Iteration 205, loss = 0.41890163\n",
      "Iteration 206, loss = 0.42010919\n",
      "Iteration 207, loss = 0.41885456\n",
      "Iteration 208, loss = 0.41923339\n",
      "Iteration 209, loss = 0.41871395\n",
      "Iteration 210, loss = 0.41915561\n",
      "Iteration 211, loss = 0.41751121\n",
      "Iteration 212, loss = 0.41803714\n",
      "Iteration 213, loss = 0.41737054\n",
      "Iteration 214, loss = 0.41805482\n",
      "Iteration 215, loss = 0.41723776\n",
      "Iteration 216, loss = 0.41700758\n",
      "Iteration 217, loss = 0.41631201\n",
      "Iteration 218, loss = 0.41719816\n",
      "Iteration 219, loss = 0.41595195\n",
      "Iteration 220, loss = 0.41620742\n",
      "Iteration 221, loss = 0.41618526\n",
      "Iteration 222, loss = 0.41678910\n",
      "Iteration 223, loss = 0.41639467\n",
      "Iteration 224, loss = 0.41539337\n",
      "Iteration 225, loss = 0.41498555\n",
      "Iteration 226, loss = 0.41566277\n",
      "Iteration 227, loss = 0.41482807\n",
      "Iteration 228, loss = 0.41492973\n",
      "Iteration 229, loss = 0.41448931\n",
      "Iteration 230, loss = 0.41445149\n",
      "Iteration 231, loss = 0.41467129\n",
      "Iteration 232, loss = 0.41370538\n",
      "Iteration 233, loss = 0.41414500\n",
      "Iteration 234, loss = 0.41340604\n",
      "Iteration 235, loss = 0.41321177\n",
      "Iteration 236, loss = 0.41305560\n",
      "Iteration 237, loss = 0.41348583\n",
      "Iteration 238, loss = 0.41288540\n",
      "Iteration 239, loss = 0.41504624\n",
      "Iteration 240, loss = 0.41260481\n",
      "Iteration 241, loss = 0.41314942\n",
      "Iteration 242, loss = 0.41213498\n",
      "Iteration 243, loss = 0.41261762\n",
      "Iteration 244, loss = 0.41196034\n",
      "Iteration 245, loss = 0.41197175\n",
      "Iteration 246, loss = 0.41162879\n",
      "Iteration 247, loss = 0.41264546\n",
      "Iteration 248, loss = 0.41120842\n",
      "Iteration 249, loss = 0.41178056\n",
      "Iteration 250, loss = 0.41099799\n",
      "Iteration 251, loss = 0.41092477\n",
      "Iteration 252, loss = 0.41162921\n",
      "Iteration 253, loss = 0.41074788\n",
      "Iteration 254, loss = 0.41147469\n",
      "Iteration 255, loss = 0.41017581\n",
      "Iteration 256, loss = 0.41063725\n",
      "Iteration 257, loss = 0.40980197\n",
      "Iteration 258, loss = 0.41159036\n",
      "Iteration 259, loss = 0.41010844\n",
      "Iteration 260, loss = 0.41047868\n",
      "Iteration 261, loss = 0.41161049\n",
      "Iteration 262, loss = 0.40913961\n",
      "Iteration 263, loss = 0.41115275\n",
      "Iteration 264, loss = 0.40887493\n",
      "Iteration 265, loss = 0.41035582\n",
      "Iteration 266, loss = 0.40884282\n",
      "Iteration 267, loss = 0.41008815\n",
      "Iteration 268, loss = 0.41077423\n",
      "Iteration 269, loss = 0.40894697\n",
      "Iteration 270, loss = 0.40959091\n",
      "Iteration 271, loss = 0.40891116\n",
      "Iteration 272, loss = 0.40844150\n",
      "Iteration 273, loss = 0.40878121\n",
      "Iteration 274, loss = 0.40818472\n",
      "Iteration 275, loss = 0.40917445\n",
      "Iteration 276, loss = 0.40833615\n",
      "Iteration 277, loss = 0.40799796\n",
      "Iteration 278, loss = 0.40787808\n",
      "Iteration 279, loss = 0.40796746\n",
      "Iteration 280, loss = 0.40716914\n",
      "Iteration 281, loss = 0.40768331\n",
      "Iteration 282, loss = 0.40762121\n",
      "Iteration 283, loss = 0.40753809\n",
      "Iteration 284, loss = 0.40628993\n",
      "Iteration 285, loss = 0.40755852\n",
      "Iteration 286, loss = 0.40675741\n",
      "Iteration 287, loss = 0.40678235\n",
      "Iteration 288, loss = 0.40675654\n",
      "Iteration 289, loss = 0.40633152\n",
      "Iteration 290, loss = 0.40613830\n",
      "Iteration 291, loss = 0.40593602\n",
      "Iteration 292, loss = 0.40586171\n",
      "Iteration 293, loss = 0.40578504\n",
      "Iteration 294, loss = 0.40672504\n",
      "Iteration 295, loss = 0.40816392\n",
      "Iteration 296, loss = 0.40571638\n",
      "Iteration 297, loss = 0.40623162\n",
      "Iteration 298, loss = 0.40621004\n",
      "Iteration 299, loss = 0.40555401\n",
      "Iteration 300, loss = 0.40542906\n",
      "Iteration 301, loss = 0.40493498\n",
      "Iteration 302, loss = 0.40541729\n",
      "Iteration 303, loss = 0.40586508\n",
      "Iteration 304, loss = 0.40558093\n",
      "Iteration 305, loss = 0.40531911\n",
      "Iteration 306, loss = 0.40588553\n",
      "Iteration 307, loss = 0.40444972\n",
      "Iteration 308, loss = 0.40425121\n",
      "Iteration 309, loss = 0.40507194\n",
      "Iteration 310, loss = 0.40401741\n",
      "Iteration 311, loss = 0.40432973\n",
      "Iteration 312, loss = 0.40395003\n",
      "Iteration 313, loss = 0.40442187\n",
      "Iteration 314, loss = 0.40405324\n",
      "Iteration 315, loss = 0.40534841\n",
      "Iteration 316, loss = 0.40343152\n",
      "Iteration 317, loss = 0.40437394\n",
      "Iteration 318, loss = 0.40335485\n",
      "Iteration 319, loss = 0.40335405\n",
      "Iteration 320, loss = 0.40291046\n",
      "Iteration 321, loss = 0.40419175\n",
      "Iteration 322, loss = 0.40277690\n",
      "Iteration 323, loss = 0.40287227\n",
      "Iteration 324, loss = 0.40267079\n",
      "Iteration 325, loss = 0.40231854\n",
      "Iteration 326, loss = 0.40237173\n",
      "Iteration 327, loss = 0.40249347\n",
      "Iteration 328, loss = 0.40212939\n",
      "Iteration 329, loss = 0.40346125\n",
      "Iteration 330, loss = 0.40229744\n",
      "Iteration 331, loss = 0.40204551\n",
      "Iteration 332, loss = 0.40312738\n",
      "Iteration 333, loss = 0.40165465\n",
      "Iteration 334, loss = 0.40208316\n",
      "Iteration 335, loss = 0.40134428\n",
      "Iteration 336, loss = 0.40245889\n",
      "Iteration 337, loss = 0.40194098\n",
      "Iteration 338, loss = 0.40163479\n",
      "Iteration 339, loss = 0.40129078\n",
      "Iteration 340, loss = 0.40094541\n",
      "Iteration 341, loss = 0.40101178\n",
      "Iteration 342, loss = 0.40114866\n",
      "Iteration 343, loss = 0.40078061\n",
      "Iteration 344, loss = 0.40065127\n",
      "Iteration 345, loss = 0.40101991\n",
      "Iteration 346, loss = 0.40077819\n",
      "Iteration 347, loss = 0.40049612\n",
      "Iteration 348, loss = 0.40108435\n",
      "Iteration 349, loss = 0.40037387\n",
      "Iteration 350, loss = 0.40093711\n",
      "Iteration 351, loss = 0.40019636\n",
      "Iteration 352, loss = 0.40034287\n",
      "Iteration 353, loss = 0.40010064\n",
      "Iteration 354, loss = 0.39996192\n",
      "Iteration 355, loss = 0.39987484\n",
      "Iteration 356, loss = 0.39992969\n",
      "Iteration 357, loss = 0.39959314\n",
      "Iteration 358, loss = 0.39970205\n",
      "Iteration 359, loss = 0.39949477\n",
      "Iteration 360, loss = 0.39976726\n",
      "Iteration 361, loss = 0.40197589\n",
      "Iteration 362, loss = 0.39943941\n",
      "Iteration 363, loss = 0.40212473\n",
      "Iteration 364, loss = 0.39947698\n",
      "Iteration 365, loss = 0.39961291\n",
      "Iteration 366, loss = 0.40013757\n",
      "Iteration 367, loss = 0.40065252\n",
      "Iteration 368, loss = 0.39810821\n",
      "Iteration 369, loss = 0.40352021\n",
      "Iteration 370, loss = 0.39970347\n",
      "Iteration 371, loss = 0.40028117\n",
      "Iteration 372, loss = 0.39818764\n",
      "Iteration 373, loss = 0.40102420\n",
      "Iteration 374, loss = 0.39838115\n",
      "Iteration 375, loss = 0.39900826\n",
      "Iteration 376, loss = 0.39799412\n",
      "Iteration 377, loss = 0.39855897\n",
      "Iteration 378, loss = 0.39856085\n",
      "Iteration 379, loss = 0.39923531\n",
      "Iteration 380, loss = 0.39816480\n",
      "Iteration 381, loss = 0.39803608\n",
      "Iteration 382, loss = 0.39725335\n",
      "Iteration 383, loss = 0.39878142\n",
      "Iteration 384, loss = 0.39724816\n",
      "Iteration 385, loss = 0.39881800\n",
      "Iteration 386, loss = 0.39767561\n",
      "Iteration 387, loss = 0.39885517\n",
      "Iteration 388, loss = 0.39733670\n",
      "Iteration 389, loss = 0.39707655\n",
      "Iteration 390, loss = 0.39736800\n",
      "Iteration 391, loss = 0.39740986\n",
      "Iteration 392, loss = 0.39828473\n",
      "Iteration 393, loss = 0.39785118\n",
      "Iteration 394, loss = 0.39794100\n",
      "Iteration 395, loss = 0.39699605\n",
      "Iteration 396, loss = 0.39683354\n",
      "Iteration 397, loss = 0.39658910\n",
      "Iteration 398, loss = 0.39703240\n",
      "Iteration 399, loss = 0.39755238\n",
      "Iteration 400, loss = 0.39665995\n",
      "Iteration 401, loss = 0.39675742\n",
      "Iteration 402, loss = 0.39634146\n",
      "Iteration 403, loss = 0.39647559\n",
      "Iteration 404, loss = 0.39627078\n",
      "Iteration 405, loss = 0.39732288\n",
      "Iteration 406, loss = 0.39549589\n",
      "Iteration 407, loss = 0.39918306\n",
      "Iteration 408, loss = 0.39589295\n",
      "Iteration 409, loss = 0.39610968\n",
      "Iteration 410, loss = 0.39576099\n",
      "Iteration 411, loss = 0.39650607\n",
      "Iteration 412, loss = 0.39575016\n",
      "Iteration 413, loss = 0.39597370\n",
      "Iteration 414, loss = 0.39593286\n",
      "Iteration 415, loss = 0.39554864\n",
      "Iteration 416, loss = 0.39570167\n",
      "Iteration 417, loss = 0.39573412\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(8, 8, 8),\n",
       "              max_iter=2000, tol=1e-06, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(8, 8, 8),\n",
       "              max_iter=2000, tol=1e-06, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(8, 8, 8),\n",
       "              max_iter=2000, tol=1e-06, verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = MLPClassifier(\n",
    "   verbose=True,\n",
    "   max_iter=2000,\n",
    "   activation=best_param['activation'],\n",
    "   hidden_layer_sizes=best_param['hidden_layer_sizes'],\n",
    "   solver=best_param['solver'],\n",
    "   tol=best_param['tol'],\n",
    ")\n",
    "neural_network.fit(X_titanic_train, y_titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = neural_network.predict(X_titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8708133971291866"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_titanic_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romulo/codes/projects/ml-titanic-survivor/.venv/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8708133971291866"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(neural_network)\n",
    "cm.fit(X_titanic_train, y_titanic_train)\n",
    "cm.score(X_titanic_test, y_titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90       266\n",
      "           1       0.87      0.76      0.81       152\n",
      "\n",
      "    accuracy                           0.87       418\n",
      "   macro avg       0.87      0.85      0.86       418\n",
      "weighted avg       0.87      0.87      0.87       418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_titanic_test, predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informa que 21 pessoas que *sobrevivem* *morrem* (FP)\n",
    "Informa que 14 pessoas que *morrem* ficam *vivas* (FN)\n",
    "\n",
    "Possui uma taxa de 91% de acertos\n",
    "\n",
    "- Acerta 92% de 0 e desses 95% estão certos\n",
    "- Acerta 91% de 1 e desses 87% estão certos "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9de851881b5172be1eb459713c1d6af298fc8f1ddbdff9c50ad6540e21843a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
